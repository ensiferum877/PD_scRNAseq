{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24bc355-e1ec-4b63-a936-d5143bc689b7",
   "metadata": {},
   "source": [
    "# Overview of the scRNA-seq Data Processing Pipeline\n",
    "\n",
    "This pipeline is designed to **load single-cell RNA sequencing (scRNA-seq) data**, perform preliminary data processing, save the data in Hierarchical Data Format (HDF5) as .h5ad files suitable for downstream analysis.\n",
    "\n",
    "Furthermore, it logs essential statistics about the processing run. \n",
    "The pipeline leverages a YAML configuration file for easy adaptability and reproducibility across different datasets and environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dae178-6038-421d-9949-6c0743afeb4d",
   "metadata": {},
   "source": [
    "# 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345cf4f5-3835-45b2-a97b-da220e344651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import os\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c952d-f75b-467b-86cf-1d92295b8357",
   "metadata": {},
   "source": [
    "# 2 - Declare functions to create a loading pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d65a1c-caff-49e7-9965-0e79aaa3b78c",
   "metadata": {},
   "source": [
    "## 2.1 - load_and_filter_scrnaseq\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Loads scRNA-seq data from specified directories, filters them based on certain identifiers, and formats them into an appropriate data structure (e.g., an AnnData object for .h5ad files).\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "parent_directory_path (str): Path to the directory containing the scRNA-seq data files.\n",
    "sample_identifiers (list): List of strings used to select specific directories or files for processing.\n",
    "exclude_dirs (list): List of directory names to exclude from processing.\n",
    "Outputs:\n",
    "\n",
    "Returns a dictionary of AnnData objects or a similar data structure, indexed by sample identifier, ready for further analysis or file output.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "Iterates over subdirectories within the parent_directory_path, excluding specified directories.\n",
    "For each subdirectory that includes any of the identifiers in sample_identifiers, it loads the data, typically using a library suited to reading the specific format of scRNA-seq data.\n",
    "Data are potentially preprocessed or filtered based on additional criteria, and then organized into a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcda28-e24b-48d3-ba06-d3905bc5c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_scrnaseq(parent_directory_path, sample_identifiers, exclude_dirs=None):\n",
    "    \"\"\"\n",
    "    Load scRNA-seq data from subdirectories containing specific identifiers and store as separate AnnData objects in dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - parent_directory_path: str, path to the parent directory containing sample subdirectories\n",
    "    - sample_identifiers: list, list of strings that subdirectory names must contain to be included\n",
    "    - exclude_dirs: list, optional list of directory names to exclude\n",
    "    \n",
    "    Returns:\n",
    "    - adatas: dict, dictionary of dictionaries of AnnData objects keyed by the identifiers and then sample names\n",
    "    \"\"\"\n",
    "    if exclude_dirs is None:\n",
    "        exclude_dirs = ['.ipynb_checkpoints', 'cache']\n",
    "    \n",
    "    # Initialize dictionary to store dictionaries of AnnData objects for each identifier\n",
    "    adatas = {identifier: {} for identifier in sample_identifiers}\n",
    "    \n",
    "    # Create a dictionary of samples that include any of the specified identifiers\n",
    "    samples = {}\n",
    "    for d in os.listdir(parent_directory_path):\n",
    "        dir_path = os.path.join(parent_directory_path, d)\n",
    "        if os.path.isdir(dir_path) and d not in exclude_dirs:\n",
    "            for identifier in sample_identifiers:\n",
    "                if identifier in d:\n",
    "                    samples.setdefault(identifier, []).append(dir_path)\n",
    "    \n",
    "    # Load data for each identifier\n",
    "    for identifier, paths in samples.items():\n",
    "        # Dictionary to store AnnData objects for the current identifier\n",
    "        adata_dict = {}\n",
    "        for path in paths:\n",
    "            sample_name = os.path.basename(path)\n",
    "            # Load the dataset\n",
    "            adata = sc.read_10x_mtx(path, var_names='gene_symbols', cache=True)\n",
    "            adata.var_names_make_unique()\n",
    "            adata_dict[sample_name] = adata\n",
    "        \n",
    "        # Store the dictionary of AnnData objects under the corresponding identifier\n",
    "        adatas[identifier] = adata_dict\n",
    "\n",
    "    return adatas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c5671-9d2c-48e9-b9b6-c6dfc2a6ce1f",
   "metadata": {},
   "source": [
    "## 2.1 - report_sample_shapes\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This function generates a summary DataFrame that reports the number of cells (observations) and features (variables such as genes) for each sample in the processed scRNA-seq data. This statistical summary is crucial for initial data quality assessment and understanding the dataset's composition.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "adatas (dict): A dictionary of AnnData objects, which are data structures commonly used in scRNA-seq analysis to store matrix data along with annotations of observations (cells) and variables (genes). The keys in this dictionary typically represent sample identifiers.\n",
    "Outputs:\n",
    "\n",
    "Returns a pandas DataFrame where each row corresponds to a sample. The DataFrame contains columns for:\n",
    "Sample: The sample identifier.\n",
    "Cells: The number of cells (observations) in the sample.\n",
    "Features: The number of features (variables, typically genes) in the sample.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "The function iterates over each key-value pair in the adatas dictionary. Each key is a sample identifier, and each value is an AnnData object.\n",
    "For each AnnData object, it retrieves the number of observations (n_obs) and variables (n_vars).\n",
    "These metrics are then used to populate a new row in the summary DataFrame.\n",
    "The populated DataFrame provides a clear and concise summary of the dataset's scale and diversity, which is essential for subsequent analytical phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d21bc3-91e7-4233-84c9-a39889140dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_sample_shapes(adatas):\n",
    "    \"\"\"\n",
    "    Generates a report on the number of cells and features for each sample in a dictionary of dictionaries of AnnData objects.\n",
    "    \n",
    "    Parameters:\n",
    "    - adatas: Dictionary where keys are identifiers and values are dictionaries of AnnData objects keyed by sample names.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with samples and their replicates as rows, and columns for the number of cells and features.\n",
    "    \"\"\"\n",
    "    # Initialize a list to hold data about each sample and its replicates\n",
    "    sample_data = []\n",
    "    \n",
    "    # Iterate through each identifier in the dictionary\n",
    "    for identifier, sample_dict in adatas.items():\n",
    "        # Process each AnnData object in the dictionary\n",
    "        for sample_name, adata in sample_dict.items():\n",
    "            num_cells = adata.n_obs\n",
    "            num_features = adata.n_vars\n",
    "            # Append this data to the list with the sample name\n",
    "            sample_data.append((f\"{identifier}_{sample_name}\", num_cells, num_features))\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    samples_df = pd.DataFrame(sample_data, columns=['Sample', 'Cells', 'Features'])\n",
    "    \n",
    "    # Set the sample name as the index of the DataFrame\n",
    "    samples_df.set_index('Sample', inplace=True)\n",
    "    \n",
    "    return samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6e49-7007-45a8-92ad-58c12e801d8f",
   "metadata": {},
   "source": [
    "## 2.3 - save_adatas_as_h5ad\n",
    "\n",
    "**Purpose:**\n",
    "Saves processed data in the .h5ad file format, which is commonly used in scRNA-seq analyses.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "adatas (dict): Dictionary of AnnData objects, indexed by sample identifier.\n",
    "directory_name (str): Path to the directory where the .h5ad files should be saved.\n",
    "Outputs:\n",
    "\n",
    "No return value. The function writes files to the disk.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "Iterates through the dictionary of AnnData objects.\n",
    "For each object, constructs a filename based on its identifier and saves it to the specified directory using the .h5ad format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29aafc0-1317-4188-aa06-49e1ef109e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_adatas_as_h5ad(adatas, directory_name='h5ad_data'):\n",
    "    \"\"\"\n",
    "    Saves each AnnData object in a nested dictionary to .h5ad files within a specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - adatas: Dictionary of dictionaries containing AnnData objects. \n",
    "              Outer keys are identifiers like 'PBMC', and inner keys are sample names like 'PBMC10_matrix'.\n",
    "    - directory_name: str, name of the directory where the .h5ad files will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each identifier and their corresponding AnnData dictionaries\n",
    "    for identifier, samples in adatas.items():\n",
    "        # Iterate through each sample name and its AnnData object\n",
    "        for sample_name, adata in samples.items():\n",
    "            # Prepare the filename by removing '_matrix' from the sample name\n",
    "            filename = sample_name.replace('_matrix', '') + '.h5ad'\n",
    "            # Define the full path where the .h5ad file will be saved\n",
    "            file_path = os.path.join(directory_name, filename)\n",
    "            # Save the AnnData object\n",
    "            adata.write_h5ad(file_path)\n",
    "            print(f\"Saved {file_path}\")  # Optional: print the path of the saved file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf86de1-f8c6-471b-9264-7c8feb29945e",
   "metadata": {},
   "source": [
    "## 2.4 - setup_logging\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Initializes the logging system for the pipeline. This function configures the logging library to write logs to a file, which helps in tracking the pipeline’s execution and debugging issues post-run.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "base_directory (str): The base directory where the log file and its parent logs directory will be created.\n",
    "log_filename (str): The name of the log file to write to within the logs directory.\n",
    "Outputs:\n",
    "\n",
    "No return value. The function sets up the logging environment used globally in the subsequent script.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "The function constructs a full path to the log file by appending the logs directory and the specified log filename to the base_directory.\n",
    "It checks if the logs directory exists and creates it if necessary.\n",
    "Logging is configured to write messages to the specified log file, with a specific format that includes the timestamp, logging level, and message. The file mode is set to overwrite (‘w’) for fresh logs each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f4a60-d520-4729-be2c-29ec75eb3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(base_directory):\n",
    "    # Create the full path for the logs directory\n",
    "    log_directory = os.path.join(base_directory, \"logs\")\n",
    "    \n",
    "    # Check if the logs directory exists, if not, create it\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory, exist_ok=True)\n",
    "    \n",
    "    # Define the full path for the log file within the logs directory\n",
    "    log_path = os.path.join(log_directory, \"pipeline.log\")\n",
    "    \n",
    "    # Set up logging to file\n",
    "    logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(\"Logging initialized - if you're seeing this, logging is configured correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b8669-c5b7-488e-b01b-acadc4bab3da",
   "metadata": {},
   "source": [
    "## 2.5 - categorize_samples\n",
    "\n",
    "**Purpose:**\n",
    "This function is designed to categorize each sample based on the presence of predefined identifiers within their names. It is typically used to classify samples into groups such as \"PBMC\" or \"CSF\", which can then be used for group-specific analyses or reporting.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "sample_name (str): The name of the sample which needs to be categorized.\n",
    "identifiers (list): A list of strings where each string is an identifier used to categorize the samples (e.g., [\"PBMC\", \"CSF\"]).\n",
    "Outputs:\n",
    "\n",
    "Returns a string that is the category identifier found in the sample_name. If no identifiers are found, it returns 'Unknown', indicating that the sample did not match any of the predefined categories.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "The function iterates through the list of identifiers.\n",
    "For each identifier, it checks if the identifier is a substring of the sample_name.\n",
    "If a match is found, it returns the identifier as the category of the sample.\n",
    "If no matches are found after checking all identifiers, it returns 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030ca2-2ee0-4949-a29b-1d4c3bba833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_samples(sample_name, identifiers):\n",
    "    for identifier in identifiers:\n",
    "        if identifier in sample_name:\n",
    "            return identifier\n",
    "    return 'Unknown'  # For samples that do not match any identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dfbe8-c3b5-4f57-b90e-60bf908b41f8",
   "metadata": {},
   "source": [
    "## 2.6 - log_additional_info\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Enhances the logging by calculating and reporting statistical metrics for each category of samples. This includes calculating the median number of cells per category and identifying outliers based on cell counts. This function aids in quickly identifying potential data quality issues or interesting biological variations.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "summary_df (DataFrame): A pandas DataFrame containing summary statistics of samples, which must include at least 'Cells' as a column and use sample names as the index.\n",
    "identifiers (list): List of identifiers used to categorize samples, which are used to group the data for median and outlier calculations.\n",
    "Outputs:\n",
    "\n",
    "No return value. This function logs detailed statistics directly to the configured log file, facilitating both immediate checks and historical record-keeping.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "Category Assignment: First, the function applies the categorize_samples to assign categories to each sample based on its name (stored in the DataFrame's index).\n",
    "Median Calculation: Computes the median number of cells for each category and logs this information to provide a quick overview of group statistics.\n",
    "Outlier Detection:\n",
    "Calculates the Interquartile Range (IQR) for cells in each category.\n",
    "Defines outliers as those samples whose cell count is significantly higher or lower than the median (using a common statistical threshold of 1.5*IQR above the third quartile and below the first quartile).\n",
    "Logs detailed information about any identified outliers for further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6b9465-3dd1-42f3-abb7-74d644855334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_additional_info(summary_df, identifiers):\n",
    "    # The index is assumed to be 'Sample', which contains the sample names\n",
    "    summary_df['Category'] = summary_df.index.to_series().apply(lambda x: categorize_samples(x, identifiers))\n",
    "    median_cells = summary_df.groupby('Category')['Cells'].median()\n",
    "    logging.info(\"Median number of cells per category:\\n\" + median_cells.to_string())\n",
    "\n",
    "    # Outlier detection\n",
    "    for category, group in summary_df.groupby('Category'):\n",
    "        median = group['Cells'].median()\n",
    "        iqr = group['Cells'].quantile(0.75) - group['Cells'].quantile(0.25)\n",
    "        upper_threshold = median + 1.5 * iqr\n",
    "        lower_threshold = median - 1.5 * iqr\n",
    "        outliers = group[(group['Cells'] > upper_threshold) | (group['Cells'] < lower_threshold)]\n",
    "        if not outliers.empty:\n",
    "            logging.info(f\"Outliers in {category} based on cell count:\\n\" + outliers.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b89dc-6b8d-4efc-9f17-0449206f7a2a",
   "metadata": {},
   "source": [
    "# 3 - Declare the wrapper function 'run_pipeline'\n",
    "\n",
    "**Purpose:**\n",
    "This function integrates various components of the scRNA-seq data processing pipeline, managing the flow from initial setup to the final outputs. It ensures that each step of the process is executed in sequence, and appropriate logs are generated for tracking and review.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "config_path (str): The file path to the YAML configuration file which contains all necessary parameters for the pipeline's execution.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "No direct return value. However, the function's execution results in several side effects including:\n",
    "Processed scRNA-seq data files saved in the .h5ad format.\n",
    "A comprehensive log file containing detailed information about the processing stages and data statistics.\n",
    "Print statements to the console indicating the completion status or any critical messages.\n",
    "Mechanism:\n",
    "\n",
    "**Configuration Loading:**\n",
    "\n",
    "Opens and reads the specified YAML configuration file.\n",
    "Loads all relevant settings into the script, which includes directories for input data and outputs, logging configurations, and parameters for data processing functions.\n",
    "- Logging Initialization:\n",
    "Calls setup_logging to configure the Python logging module to write all logs to a specified file. This setup includes creating a logging directory if it does not exist.\n",
    "- Directory Preparation:\n",
    "Ensures that the output directory for .h5ad files exists, creating it if necessary.\n",
    "Data Loading and Processing:\n",
    "Executes load_and_filter_scrnaseq, which filters and loads data from specified directories into memory as structured AnnData objects. This step involves reading the data, applying predefined filters, and possibly some initial preprocessing.\n",
    "- Data Summarization:\n",
    "Processes the loaded data to generate a summary DataFrame that includes statistics like the number of cells and features per sample using report_sample_shapes.\n",
    "- Additional Logging:\n",
    "Further logs detailed statistics such as median cell counts per category and outliers using log_additional_info.\n",
    "- Data Saving:\n",
    "Saves the processed data into .h5ad files in the designated output directory through save_adatas_as_h5ad.\n",
    "- Completion Confirmation:\n",
    "Prints a message to the console to confirm that all samples have been successfully processed and saved, providing instant feedback on the operation's success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e393a-6ed1-4d03-b60a-036d42aab15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Initialize logging using the parent directory path\n",
    "    setup_logging(config['pipeline_settings']['parent_directory_path'])\n",
    "    logging.info(\"Configuration loaded successfully\")\n",
    "\n",
    "    # Ensure the output directory is ready\n",
    "    os.makedirs(config['pipeline_settings']['output_directory'], exist_ok=True)\n",
    "    logging.info(f\"Output directory {config['pipeline_settings']['output_directory']} prepared.\")\n",
    "\n",
    "    # Process data\n",
    "    adatas = load_and_filter_scrnaseq(\n",
    "        parent_directory_path=config['pipeline_settings']['parent_directory_path'],\n",
    "        sample_identifiers=config['pipeline_settings']['sample_identifiers'],\n",
    "        exclude_dirs=config['pipeline_settings']['exclude_dirs']\n",
    "    )\n",
    "    logging.info(\"Data loaded and filtered successfully\")\n",
    "\n",
    "    # Calculate and log sample information\n",
    "    total_samples = 0\n",
    "    for identifier, samples in adatas.items():\n",
    "        num_samples = len(samples)\n",
    "        total_samples += num_samples\n",
    "        logging.info(f\"Processed {num_samples} samples for identifier '{identifier}'\")\n",
    "\n",
    "    # Generate and log the summary DataFrame\n",
    "    summary_df = report_sample_shapes(adatas)\n",
    "    logging.info(\"Summary DataFrame of cells and features per sample:\\n\" + summary_df.to_string(index=True))\n",
    "\n",
    "    # Log additional information including median and outliers\n",
    "    log_additional_info(summary_df, config['pipeline_settings']['sample_identifiers'])\n",
    "\n",
    "    # Save processed data\n",
    "    save_adatas_as_h5ad(adatas, directory_name=config['pipeline_settings']['output_directory'])\n",
    "    logging.info(\"All data saved as .h5ad files.\")\n",
    "\n",
    "    # Final user message\n",
    "    print(config['output_settings']['dataframe_print_message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a006e-7562-47c0-a6bd-49296d771738",
   "metadata": {},
   "source": [
    "# 4 - Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037ddfb5-81c4-4681-adf0-9bfd718631fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved h5ad_data/PBMC10.h5ad\n",
      "Saved h5ad_data/PBMC24.h5ad\n",
      "Saved h5ad_data/PBMC28.h5ad\n",
      "Saved h5ad_data/PBMC8.h5ad\n",
      "Saved h5ad_data/PBMC32.h5ad\n",
      "Saved h5ad_data/PBMC20.h5ad\n",
      "Saved h5ad_data/PBMC14.h5ad\n",
      "Saved h5ad_data/PBMC30.h5ad\n",
      "Saved h5ad_data/PBMC27.h5ad\n",
      "Saved h5ad_data/PBMC25.h5ad\n",
      "Saved h5ad_data/PBMC21.h5ad\n",
      "Saved h5ad_data/PBMC23.h5ad\n",
      "Saved h5ad_data/PBMC31.h5ad\n",
      "Saved h5ad_data/CSF1.h5ad\n",
      "Saved h5ad_data/CSF31.h5ad\n",
      "Saved h5ad_data/CSF23.h5ad\n",
      "Saved h5ad_data/CSF21.h5ad\n",
      "Saved h5ad_data/CSF3.h5ad\n",
      "Saved h5ad_data/CSF29.h5ad\n",
      "Saved h5ad_data/CSF25.h5ad\n",
      "Saved h5ad_data/CSF13.h5ad\n",
      "Saved h5ad_data/CSF7.h5ad\n",
      "Saved h5ad_data/CSF27.h5ad\n",
      "Saved h5ad_data/CSF30.h5ad\n",
      "Saved h5ad_data/CSF14.h5ad\n",
      "Saved h5ad_data/CSF20.h5ad\n",
      "Saved h5ad_data/CSF32.h5ad\n",
      "Saved h5ad_data/CSF16.h5ad\n",
      "Saved h5ad_data/CSF12.h5ad\n",
      "Saved h5ad_data/CSF28.h5ad\n",
      "Saved h5ad_data/CSF24.h5ad\n",
      "Saved h5ad_data/CSF10.h5ad\n",
      "Saved h5ad_data/CSF26.h5ad\n",
      "Saved h5ad_data/CSF8.h5ad\n",
      "Saved h5ad_data/CSF4.h5ad\n",
      "All samples have been successfully processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "config_path = 'config.yaml'\n",
    "run_pipeline(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106c857-ad87-4942-bc9c-c9a278b90197",
   "metadata": {},
   "source": [
    "# 5 - Parent directory structure\n",
    "\n",
    "The directory tree depicted below meticulously outlines the organized structure of this project.\n",
    "\n",
    "This structure is pivotal for maintaining an efficient workflow and facilitates the initial setup, processing of scRNA-seq data, and subsequent analysis steps. Here’s a detailed breakdown of the directory tree and the functional purpose of each component:\n",
    "\n",
    "**Initial Data Structure**\n",
    "\n",
    "Before running the pipeline, the data should be organized according to the structure shown under the data/ directory. Each primary category (e.g., PBMC, CSF) represents a distinct dataset or experimental condition. \n",
    "\n",
    "PBMC/ and CSF/: These directories  house the data files necessary for processing (counts.txt, metadata.txt, matrix.mtx). This setup illustrates the typical segregation of scRNA-seq data by sample type or condition, facilitating targeted analyses and batch processing.\n",
    "\n",
    "**Post-Pipeline Structure**\n",
    "\n",
    "After executing the pipeline, additional directories and files are generated, structured as follows:\n",
    "\n",
    "\n",
    "h5ad_data/: This directory is populated with processed data files, typically in .h5ad format, which are ready for downstream analysis or publication. The organization within this directory can be customized based on user needs or specific analysis stages.\n",
    "\n",
    "logs/: Stores log files that record detailed execution traces, important for debugging and verifying pipeline operations. The pipeline.log file, for instance, provides insights into each step of the pipeline, including any issues encountered and statistics on the data processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e5560e-9e36-49f7-8f1c-b3aec1dc0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── .ipynb_checkpoints\n",
      "│   ├── Untitled-checkpoint.ipynb\n",
      "│   ├── config-checkpoint.yaml\n",
      "│   └── scRNA-seq_PD_PBMC-checkpoint.ipynb\n",
      "├── CSF10_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .barcodes.tsv.icloud\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   └── genes.tsv\n",
      "├── CSF12_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF13_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF14_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .barcodes.tsv.icloud\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   └── genes.tsv\n",
      "├── CSF16_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF1_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF20_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF21_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .barcodes.tsv.icloud\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   └── genes.tsv\n",
      "├── CSF23_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF24_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .barcodes.tsv.icloud\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   └── genes.tsv\n",
      "├── CSF25_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF26_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF27_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF28_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF29_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF30_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF31_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF32_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── CSF3_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF4_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── CSF7_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   ├── .barcodes.tsv.icloud\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   └── genes.tsv\n",
      "├── CSF8_matrix\n",
      "│   ├── .matrix.mtx.icloud\n",
      "│   ├── barcodes.tsv\n",
      "│   └── genes.tsv\n",
      "├── PBMC10_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   │   └── barcodes-checkpoint.tsv\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC14_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC20_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC21_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC23_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC24_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC25_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC27_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC28_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC30_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC31_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC32_matrix\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── PBMC8_matrix\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   │   └── barcodes-checkpoint.tsv\n",
      "│   ├── barcodes.tsv\n",
      "│   ├── genes.tsv\n",
      "│   └── matrix.mtx\n",
      "├── cache\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF10_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF12_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF13_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF14_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF16_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF1_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF20_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF21_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF23_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF24_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF25_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF26_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF27_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF28_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF29_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF30_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF31_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF32_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF3_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF4_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF7_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-CSF8_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC10_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC14_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC20_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC21_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC23_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC24_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC25_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC27_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC28_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC30_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC31_matrix-matrix.h5ad\n",
      "│   ├── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC32_matrix-matrix.h5ad\n",
      "│   └── Users-felipe-Desktop-Portfolio_FNV-PD_scRNAseq_PBMC-PBMC8_matrix-matrix.h5ad\n",
      "├── h5ad_data\n",
      "│   ├── CSF1.h5ad\n",
      "│   ├── CSF10.h5ad\n",
      "│   ├── CSF12.h5ad\n",
      "│   ├── CSF13.h5ad\n",
      "│   ├── CSF14.h5ad\n",
      "│   ├── CSF16.h5ad\n",
      "│   ├── CSF20.h5ad\n",
      "│   ├── CSF21.h5ad\n",
      "│   ├── CSF23.h5ad\n",
      "│   ├── CSF24.h5ad\n",
      "│   ├── CSF25.h5ad\n",
      "│   ├── CSF26.h5ad\n",
      "│   ├── CSF27.h5ad\n",
      "│   ├── CSF28.h5ad\n",
      "│   ├── CSF29.h5ad\n",
      "│   ├── CSF3.h5ad\n",
      "│   ├── CSF30.h5ad\n",
      "│   ├── CSF31.h5ad\n",
      "│   ├── CSF32.h5ad\n",
      "│   ├── CSF4.h5ad\n",
      "│   ├── CSF7.h5ad\n",
      "│   ├── CSF8.h5ad\n",
      "│   ├── PBMC10.h5ad\n",
      "│   ├── PBMC14.h5ad\n",
      "│   ├── PBMC20.h5ad\n",
      "│   ├── PBMC21.h5ad\n",
      "│   ├── PBMC23.h5ad\n",
      "│   ├── PBMC24.h5ad\n",
      "│   ├── PBMC25.h5ad\n",
      "│   ├── PBMC27.h5ad\n",
      "│   ├── PBMC28.h5ad\n",
      "│   ├── PBMC30.h5ad\n",
      "│   ├── PBMC31.h5ad\n",
      "│   ├── PBMC32.h5ad\n",
      "│   └── PBMC8.h5ad\n",
      "├── logs\n",
      "│   ├── .ipynb_checkpoints\n",
      "│   │   └── pipeline-checkpoint.log\n",
      "│   └── pipeline.log\n",
      "├── .DS_Store\n",
      "├── Untitled.ipynb\n",
      "├── config.yaml\n",
      "└── scRNA-seq_PD_PBMC.ipynb\n"
     ]
    }
   ],
   "source": [
    "def print_tree(directory, prefix=''):\n",
    "    \"\"\"Recursively prints a formatted tree structure of the directory path given.\n",
    "\n",
    "    Args:\n",
    "    directory (str): Path of the directory to print the structure for.\n",
    "    prefix (str): Prefix to use for line formatting in recursive calls.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    directories = []\n",
    "    # Separate files and directories\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            directories.append(item)\n",
    "        else:\n",
    "            files.append(item)\n",
    "\n",
    "    # Sort directories and files for consistent order\n",
    "    directories.sort()\n",
    "    files.sort()\n",
    "\n",
    "    # Print directories and files\n",
    "    entries = directories + files\n",
    "    for i, entry in enumerate(entries):\n",
    "        connector = \"└──\" if i == len(entries) - 1 else \"├──\"\n",
    "        if i == len(entries) - 1:\n",
    "            next_prefix = prefix + \"    \"\n",
    "        else:\n",
    "            next_prefix = prefix + \"│   \"\n",
    "\n",
    "        print(f\"{prefix}{connector} {entry}\")\n",
    "        if entry in directories:\n",
    "            new_dir = os.path.join(directory, entry)\n",
    "            print_tree(new_dir, prefix=next_prefix)\n",
    "\n",
    "# Example usage:\n",
    "print_tree('/Users/felipe/Desktop/Portfolio_FNV/PD_scRNAseq_PBMC/') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f545a-8c3d-4172-a19a-85b13f80d107",
   "metadata": {},
   "source": [
    "# 6 - Improvement suggestions for next versions of this pipeline\n",
    "\n",
    "**Handling No Specific Substrings**\n",
    "\n",
    "To handle situations where **no specific substrings** are provided (i.e., no cell type names in the folder names):\n",
    "\n",
    "Configuration Setup: Define a default behavior when sample_identifiers is empty or a specific keyword like 'ALL' is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
